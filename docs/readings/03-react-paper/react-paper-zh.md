# ReAct: 在语言模型中协同推理与行动

> 论文地址: https://arxiv.org/abs/2210.03629
> 作者: Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao
> 发表时间: 2022年10月 (ICLR 2023)
> 项目主页: https://react-lm.github.io/

---

## 摘要

虽然大型语言模型（LLMs）在语言理解和交互式决策任务中展示了令人印象深刻的能力，但它们的推理能力（如思维链提示）和行动能力（如行动计划生成）主要被作为独立的主题研究。在本文中，我们探索使用 LLMs 以**交错的方式**生成**推理轨迹**和**特定任务的行动**，从而在两者之间实现更大的协同作用：

- **推理轨迹**帮助模型归纳、跟踪和更新行动计划，以及处理异常
- **行动**使其能够与外部源（如知识库或环境）交互，以收集额外信息

我们将我们的方法命名为 **ReAct**，应用于多种语言和决策任务，并展示了其相对于最先进基线的有效性，以及相对于没有推理或行动组件的方法改进的人类可解释性和可信度。

---

## 1. 引言

### 分离推理与行动的问题

人类拥有将任务导向行动与语言推理（或内心独白）无缝结合的卓越能力。考虑在厨房做饭的例子：

- 在行动之间，我们可能用语言推理（"既然所有东西都切好了，我应该把水烧开"）
- 我们也可能采取行动来支持推理（"我需要阅读食谱来弄清楚配料用量"）

行动和推理的紧密整合使人类能够：
- 快速学习新任务
- 进行稳健的决策或推理
- 甚至即时调整计划（例如，"我没有盐，所以让我用酱油代替"）

### 以往的方法

| 方法 | 局限性 |
|------|--------|
| **思维链（CoT）** | 使用 LLM 的内部知识，可能产生幻觉 |
| **行动生成** | 生成行动但没有高级推理 |

### ReAct 解决方案

ReAct 提示 LLMs 生成：
1. **语言推理轨迹** - 用于模型推理任务解决
2. **行动** - 与外部环境交互（例如，Wikipedia API）

这使模型能够：
- 归纳、跟踪和更新行动计划
- 处理异常情况
- 与外部源交互以获取额外信息

---

## 2. ReAct 范式

### 核心组件

```
┌─────────────────────────────────────────────────────────────┐
│                     ReAct Agent 循环                         │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│   观察 (Observation) ──► 思考 (Thought) ──► 行动 (Action)   │
│       ↑                                        │             │
│       └────────────────────────────────────────┘             │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 三个要素

| 要素 | 描述 | 示例 |
|------|------|------|
| **思考 (Thought)** | 关于当前情况的推理 | "我需要搜索关于X的信息" |
| **行动 (Action)** | 与外部环境的交互 | `Search[X]`, `Lookup[term]`, `Finish[answer]` |
| **观察 (Observation)** | 来自环境的结果 | 搜索结果、查找结果 |

### ReAct 轨迹示例 (HotpotQA)

```
问题：科罗拉多造山运动的东部扇区延伸到的区域的海拔范围是多少？

思考 1：我需要搜索科罗拉多造山运动，找到科罗拉多造山运动东部扇区延伸到的区域，
然后找到该区域的海拔范围。

行动 1：Search[科罗拉多造山运动]

观察 1：科罗拉多造山运动是科罗拉多及周边地区的一次造山运动。

思考 2：没有提到东部扇区。所以我需要查找东部扇区。

行动 2：Lookup[东部扇区]

观察 2：（结果 1 / 1）东部扇区延伸到高原地区。

思考 3：科罗拉多造山运动的东部扇区延伸到高原地区。
所以我需要搜索高原地区并找到其海拔范围。

行动 3：Search[高原地区]

观察 3：高原地区指的是两个不同陆地区域之一。

思考 4：我需要改为搜索美国高原地区。

行动 4：Search[美国高原地区]

观察 4：高原地区是大平原的一个子区域。
从东到西，高原地区的海拔从约 1,800 英尺上升到 7,000 英尺（550 到 2,130 米）。

思考 5：高原地区的海拔从约 1,800 英尺上升到 7,000 英尺，
所以答案是 1,800 到 7,000 英尺。

行动 5：Finish[1,800 到 7,000 英尺]
```

---

## 3. 行动空间

### 知识密集型任务（HotpotQA, Fever）

| 行动 | 描述 |
|------|------|
| `Search[实体]` | 如果存在，返回 Wikipedia 页面的前 5 句话，或建议相似实体 |
| `Lookup[字符串]` | 返回当前页面中包含该字符串的下一句话 |
| `Finish[答案]` | 返回答案并完成任务 |

### 决策任务（ALFWorld, WebShop）

基于环境的特定领域行动：
- ALFWorld：`go to`（前往）, `pick up`（拿起）, `put`（放置）, `open`（打开）, `close`（关闭）, `use`（使用）等
- WebShop：`search`（搜索）, `click`（点击）, `buy`（购买）等

---

## 4. ReAct vs 其他方法

### 对比表

| 方法 | 推理 | 行动 | 外部知识 | 可解释 |
|------|------|------|----------|--------|
| 标准提示 | ❌ | ❌ | ❌ | ❌ |
| 思维链（CoT） | ✅ | ❌ | ❌ | ✅ |
| 仅行动 | ❌ | ✅ | ✅ | ❌ |
| **ReAct** | ✅ | ✅ | ✅ | ✅ |

### ReAct 的主要优势

1. **减少幻觉**：通过在外部知识中锚定推理
2. **可解释性**：展示模型的推理过程
3. **灵活性**：可以通过调整计划从错误中恢复
4. **泛化性**：适用于不同类型的任务

---

## 5. 实验结果

### HotpotQA（问答任务）

| 方法 | 精确匹配 |
|------|----------|
| 标准 | 28.7% |
| CoT | 29.4% |
| 仅行动 | 25.7% |
| **ReAct** | **34.3%** |
| ReAct + CoT-SC | **35.1%** |

### Fever（事实验证）

| 方法 | 准确率 |
|------|--------|
| 标准 | 57.1% |
| CoT | 56.3% |
| 仅行动 | 58.9% |
| **ReAct** | **60.9%** |

### ALFWorld（交互式决策）

| 方法 | 成功率 |
|------|--------|
| BUTLER（模仿学习） | 37% |
| 仅行动 | 45% |
| **ReAct** | **71%** |

### WebShop（网页导航）

| 方法 | 成功率 |
|------|--------|
| IL + RL | 29% |
| 仅行动 | 31% |
| **ReAct** | **40%** |

---

## 6. 错误分析

### 错误类型

| 错误类型 | CoT | ReAct |
|----------|-----|-------|
| 幻觉 | 高 | 低 |
| 推理错误 | 中 | 中 |
| 搜索失败 | 不适用 | 存在 |
| 标签歧义 | 存在 | 存在 |

### ReAct 的错误恢复

```
思考：搜索没有返回有用的结果。让我尝试不同的查询。
行动：Search[替代查询]
```

ReAct 能够：
- 检测信息何时不足
- 重新构建查询
- 尝试替代方法

---

## 7. 协同效应：ReAct + CoT

### 两全其美

```
ReAct + CoT 自一致性：
1. 运行 ReAct 获取外部知识
2. 运行 CoT 进行内部推理
3. 使用多数投票确定最终答案
```

这种组合在知识密集型任务上取得了最佳结果。

---

## 8. 提示设计

### 少样本提示策略

论文在 HotpotQA 上使用 6 个示例，在 Fever 上使用 3 个示例，手动标注包括：
- 多样的推理模式
- 不同的行动类型
- 错误恢复示例

### 提示模板结构

```
[任务描述]

[示例 1]
问题：...
思考 1：...
行动 1：...
观察 1：...
...

[示例 2]
...

[实际问题]
问题：{用户问题}
思考 1：
```

---

## 9. 关键见解

### ReAct 为何有效

1. **锚定推理**：行动提供真实世界反馈
2. **显式规划**：思考使计划可见且可调整
3. **错误处理**：可以检测和从错误中恢复
4. **可解释性**：人类可以理解和信任过程

### 局限性

1. **提示工程**：需要仔细设计示例
2. **API 依赖**：依赖外部 API 质量
3. **计算成本**：由于推理轨迹需要更多 token
4. **错误传播**：错误的搜索可能使推理偏离

---

## 10. 实现技巧

### 开发者指南

```python
# 简化的 ReAct 循环
def react_agent(question, tools, max_steps=10):
    context = f"问题：{question}\n"
    
    for step in range(max_steps):
        # 生成思考和行动
        response = llm.generate(context + f"思考 {step+1}：")
        thought, action = parse_response(response)
        
        context += f"思考 {step+1}：{thought}\n"
        context += f"行动 {step+1}：{action}\n"
        
        # 执行行动
        if action.startswith("Finish"):
            return extract_answer(action)
        
        observation = execute_action(action, tools)
        context += f"观察 {step+1}：{observation}\n"
    
    return "达到最大步数"
```

### 最佳实践

1. **清晰的行动定义**：明确定义可用的行动
2. **良好的示例**：包含多样、有代表性的示例
3. **错误处理**：展示错误恢复的示例
4. **观察限制**：截断长观察以节省 token

---

## 11. 结论

ReAct 证明了：
- **推理和行动在 LLMs 中是协同的**
- **外部锚定减少幻觉**
- **可解释的轨迹提高信任**
- **简单的提示可以取得强大的结果**

该方法现已被广泛应用于 LangChain、AutoGPT 等 Agent 框架中。

---

## 参考资料

- 原始论文：https://arxiv.org/abs/2210.03629
- 项目网站：https://react-lm.github.io/
- 相关论文：思维链提示（Wei et al., 2022）
