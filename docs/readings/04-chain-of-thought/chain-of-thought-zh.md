# 思维链提示：激发大型语言模型的推理能力

> 论文地址: https://arxiv.org/abs/2201.11903
> 作者: Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou
> 发表时间: 2022年1月 (NeurIPS 2022)

---

## 摘要

我们探索了生成**思维链**——一系列中间推理步骤——如何显著提高大型语言模型执行复杂推理的能力。特别是，我们展示了这种推理能力如何通过一种称为**思维链提示**的简单方法在足够大的语言模型中自然涌现，其中只需在提示中提供几个思维链演示作为示例。

在三个大型语言模型上的实验表明，思维链提示提高了在一系列算术、常识和符号推理任务上的性能。实证收益可能是惊人的——例如，**使用仅8个思维链示例提示540B参数的语言模型在GSM8K数学应用题基准上达到了最先进的准确率**，甚至超过了带验证器的微调GPT-3。

---

## 1. 引言

### 复杂推理的挑战

虽然大型语言模型（LLMs）在许多NLP任务上取得了显著成功，但它们在需要多步推理的任务上仍然存在困难，例如：

- **数学推理**：解决应用题
- **常识推理**：理解日常场景
- **符号推理**：遵循逻辑规则

### 解决方案：思维链

不是直接输出答案，而是提示模型产生**一系列导向最终答案的中间推理步骤**。

```
标准提示：
问：罗杰有5个网球。他又买了2罐网球。每罐有3个网球。他现在有多少个网球？
答：11

思维链提示：
问：罗杰有5个网球。他又买了2罐网球。每罐有3个网球。他现在有多少个网球？
答：罗杰开始有5个球。2罐网球，每罐3个，就是6个网球。5 + 6 = 11。答案是11。
```

---

## 2. 思维链提示

### 定义

**思维链（CoT）**提示是一种技术，鼓励模型在得出最终答案之前生成中间推理步骤。

### 关键特性

| 特性 | 描述 |
|------|------|
| **分解** | 将复杂问题分解成更小的步骤 |
| **可解释性** | 使模型的推理过程可见 |
| **涌现能力** | 仅在足够大的模型中出现 |
| **少样本** | 只需要几个演示示例 |

### 工作原理

```
┌─────────────────────────────────────────────────────────┐
│                   CoT 提示流程                           │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  [包含推理的少样本示例]                                   │
│           ↓                                              │
│  [新问题]                                                │
│           ↓                                              │
│  [模型生成推理步骤]                                       │
│           ↓                                              │
│  [最终答案]                                              │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

---

## 3. 少样本思维链

### 基本方法

为模型提供几个包含逐步推理的示例：

```
示例提示：

问：餐厅有23个苹果。如果他们用了20个做午餐，又买了6个，他们有多少个苹果？
答：餐厅原来有23个苹果。他们用了20个做午餐。所以他们有 23 - 20 = 3 个。
    他们又买了6个苹果，所以他们有 3 + 6 = 9 个。答案是9。

问：我去市场买了10个苹果。我给了邻居2个苹果，给了修理工2个。
    然后我又去买了5个苹果，吃了1个。我还剩多少个苹果？
答：[模型生成推理和答案]
```

### 示例：算术推理

**没有 CoT：**
```
问：一个杂技演员可以玩16个球。一半的球是高尔夫球，
   一半的高尔夫球是蓝色的。有多少个蓝色高尔夫球？
答：8（错误）
```

**有 CoT：**
```
问：一个杂技演员可以玩16个球。一半的球是高尔夫球，
   一半的高尔夫球是蓝色的。有多少个蓝色高尔夫球？
答：16个球的一半是8个高尔夫球。
   8个高尔夫球的一半是4个蓝色高尔夫球。
   答案是4。（正确）
```

---

## 4. 零样本思维链

### 神奇的短语

在提示中添加简单的短语 **"让我们一步一步思考"（Let's think step by step）** 可以在没有任何示例的情况下触发思维链推理！

> 参考："Large Language Models are Zero-Shot Reasoners"（Kojima et al., 2022）

### 工作原理

```
标准零样本：
问：一个杂技演员可以玩16个球。一半的球是高尔夫球，
   一半的高尔夫球是蓝色的。有多少个蓝色高尔夫球？
答：8（错误）

零样本 CoT：
问：一个杂技演员可以玩16个球。一半的球是高尔夫球，
   一半的高尔夫球是蓝色的。有多少个蓝色高尔夫球？
让我们一步一步思考。
答：首先，我们知道杂技演员可以玩16个球。
   16的一半是8，所以有8个高尔夫球。
   高尔夫球（8个）的一半是蓝色的，也就是4个。
   因此，有4个蓝色高尔夫球。（正确）
```

### 有效的触发短语

| 短语 | 有效性 |
|------|--------|
| "Let's think step by step"（让我们一步一步思考） | ✅ 最有效 |
| "Let's work this out in a step by step way"（让我们一步一步解决这个问题） | ✅ 非常有效 |
| "First, let me think about this carefully"（首先，让我仔细想想这个问题） | ✅ 有效 |
| "Think step by step"（一步一步思考） | ⚠️ 效果较差 |

---

## 5. 自动思维链（Auto-CoT）

### 手动 CoT 的问题

手动创建思维链演示：
- 耗时
- 需要领域专业知识
- 可能泛化性差

### Auto-CoT 解决方案

> 参考："Automatic Chain of Thought Prompting in Large Language Models"（Zhang et al., 2022）

Auto-CoT 通过以下方式自动构建演示：

1. **聚类**：将问题分组
2. **选择**：从每个聚类中选择代表性问题
3. **生成**：使用零样本 CoT 生成推理链

```
┌─────────────────────────────────────────────────────────┐
│                   Auto-CoT 流程                          │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  [问题池]                                                │
│       ↓                                                  │
│  [按相似性聚类]                                          │
│       ↓                                                  │
│  [选择代表性问题]                                        │
│       ↓                                                  │
│  [通过零样本 CoT 生成推理过程]                            │
│       ↓                                                  │
│  [自动生成的演示]                                        │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

### Auto-CoT 的优势

| 优势 | 描述 |
|------|------|
| **无需手动工作** | 自动生成演示 |
| **多样性** | 从不同问题聚类中采样 |
| **可扩展性** | 适用于不同领域 |
| **性能** | 在许多情况下匹配或超过手动 CoT |

---

## 6. 自一致性

### 超越贪婪解码

**自一致性**通过以下方式改进 CoT：
1. 采样**多个**推理路径
2. 对最终答案进行**多数投票**

> 参考："Self-Consistency Improves Chain of Thought Reasoning in Language Models"（Wang et al., 2022）

### 工作原理

```
问题：如果停车场有3辆车，又来了2辆车，停车场有多少辆车？

路径 1：有3辆车。又来了2辆。3 + 2 = 5。答案：5
路径 2：从3开始，加2，得到5。答案：5
路径 3：3辆车 + 2辆车 = 5辆车。答案：5

多数投票 → 答案：5
```

### 性能提升

| 任务 | CoT | CoT + 自一致性 |
|------|-----|----------------|
| GSM8K | 57% | 74% |
| SVAMP | 79% | 86% |
| AQuA | 48% | 58% |

---

## 7. 实验结果

### 算术推理

| 模型 + 方法 | GSM8K | SVAMP | ASDiv |
|-------------|-------|-------|-------|
| GPT-3（标准） | 15.6% | 66.4% | 71.3% |
| GPT-3（CoT） | 46.9% | 74.5% | 76.9% |
| PaLM 540B（CoT） | **58.1%** | **79.0%** | **80.4%** |

### 常识推理

| 模型 + 方法 | CommonsenseQA | StrategyQA |
|-------------|---------------|------------|
| GPT-3（标准） | 73.0% | 63.4% |
| GPT-3（CoT） | 78.0% | 73.4% |

### 符号推理

| 任务 | 标准 | 思维链 |
|------|------|--------|
| 最后一个字母（2个词） | 1.4% | 99.6% |
| 最后一个字母（4个词） | 0.2% | 98.8% |
| 抛硬币（4次） | 53.8% | 100% |

---

## 8. CoT 为什么有效？

### 关键见解

1. **分解**：将复杂问题分解为更简单的子问题
2. **显式计算**：允许模型"展示其工作"
3. **错误定位**：更容易识别推理出错的地方
4. **注意力引导**：将模型的注意力引向相关信息

### 涌现能力

CoT 只对**足够大的模型**有帮助：

| 模型大小 | CoT 收益 |
|----------|----------|
| < 100亿参数 | 几乎没有改进 |
| 100亿 - 1000亿参数 | 中等改进 |
| > 1000亿参数 | 显著改进 |

---

## 9. 局限性

### CoT 失败的情况

1. **简单任务**：额外开销但无收益
2. **知识空白**：无法推理未知事实
3. **计算错误**：仍然会犯算术错误
4. **幻觉**：可能生成看似合理但错误的推理

### 失败示例

```
问：27 × 43 等于多少？
答：让我们一步一步思考。
   27 × 43 = 27 × 40 + 27 × 3
   = 1080 + 81
   = 1161（正确答案：1161）

问：1847 × 9428 等于多少？
答：让我们一步一步思考。
   1847 × 9428 ≈ 17,000,000（错误 - 正确答案：17,417,516）
```

---

## 10. 最佳实践

### 提示设计技巧

| 技巧 | 描述 |
|------|------|
| **清晰的步骤** | 使用编号或顺序推理 |
| **展示所有工作** | 包括中间计算 |
| **一致的格式** | 在所有示例中使用相同的结构 |
| **多样的示例** | 涵盖不同的问题类型 |
| **验证逻辑** | 确保示例推理正确 |

### 示例模板

```
问：[问题]
答：让我们一步一步解决这个问题：
   第1步：[第一个推理步骤]
   第2步：[第二个推理步骤]
   第3步：[第三个推理步骤]
   因此，答案是 [最终答案]。
```

### 组合技术

```
最佳性能 = CoT + 自一致性 + 更大的模型
```

---

## 11. 应用场景

### CoT 的使用场景

| 领域 | 应用 |
|------|------|
| **教育** | 带有逐步解答的数学辅导 |
| **客户支持** | 逻辑故障排除 |
| **法律** | 带有推理的案例分析 |
| **医疗** | 诊断推理 |
| **编程** | 算法设计和调试 |

### 与 Agent 的集成

CoT 通常与其他技术结合使用：
- **ReAct**：CoT + 行动
- **思维树**：CoT + 搜索
- **Self-Ask**：CoT + 分解

---

## 12. 代码示例

### 简单的 CoT 实现

```python
def chain_of_thought_prompt(question: str, examples: list[dict]) -> str:
    """创建带有示例的 CoT 提示。"""
    prompt = ""
    
    # 添加少样本示例
    for ex in examples:
        prompt += f"问：{ex['question']}\n"
        prompt += f"答：{ex['reasoning']}\n\n"
    
    # 添加实际问题
    prompt += f"问：{question}\n"
    prompt += "答：让我们一步一步思考。\n"
    
    return prompt

# 使用示例
examples = [
    {
        "question": "罗杰有5个网球。他又买了2罐，每罐3个。他有多少个？",
        "reasoning": "罗杰开始有5个球。他买了2罐，每罐3个，就是 2 × 3 = 6 个球。总共：5 + 6 = 11 个球。答案是11。"
    }
]

question = "莎拉有8块饼干。她给了朋友3块，又烤了5块。她有多少块？"
prompt = chain_of_thought_prompt(question, examples)
response = llm.generate(prompt)
```

### 零样本 CoT

```python
def zero_shot_cot(question: str) -> str:
    """简单的零样本 CoT 提示。"""
    prompt = f"""问：{question}

让我们一步一步思考。"""
    
    return llm.generate(prompt)
```

---

## 13. 总结

### 关键要点

| 方面 | 描述 |
|------|------|
| **核心思想** | 生成中间推理步骤 |
| **少样本 CoT** | 提供带有推理的示例 |
| **零样本 CoT** | 使用"让我们一步一步思考" |
| **Auto-CoT** | 自动生成演示 |
| **自一致性** | 采样多条路径，投票决定答案 |
| **涌现性** | 仅在大型模型中效果好 |

### 影响

思维链提示已成为现代 LLM 应用的基础技术，使得：
- 在复杂任务上更好地推理
- 更可解释的 AI 输出
- 为 ReAct 和思维树等高级技术奠定基础

---

## 参考资料

- 原始论文：https://arxiv.org/abs/2201.11903
- 零样本 CoT：https://arxiv.org/abs/2205.11916
- Auto-CoT：https://arxiv.org/abs/2210.03493
- 自一致性：https://arxiv.org/abs/2203.11171
- 提示指南：https://www.promptingguide.ai/techniques/cot
